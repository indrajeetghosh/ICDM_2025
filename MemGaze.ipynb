{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb60b66-c553-408c-92e6-68b3ed07a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from ptflops import get_model_complexity_info\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "import torchvision.models as models\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import os, random, cv2, numpy as np, torch, torch.nn as nn\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib as mpl\n",
    "import pylab\n",
    "mpl.rcParams['lines.linewidth'] = 4\n",
    "mpl.rcParams['lines.color'] = 'r'\n",
    "mpl.rcParams['font.weight'] = 200\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('figure',figsize=(15,9))\n",
    "mpl.axes.Axes.annotate\n",
    "mpl.rcParams['font.family'] = \"serif\"\n",
    "pylab.rcParams['ytick.major.pad']='15'\n",
    "pylab.rcParams['xtick.major.pad']='15'\n",
    "mpl.rcParams['font.weight'] = \"semibold\"\n",
    "mpl.rcParams['axes.labelsize'] = 25\n",
    "mpl.rcParams['axes.linewidth'] = 4\n",
    "mpl.rcParams['xtick.labelsize'] = 25\n",
    "mpl.rcParams['ytick.labelsize'] = 25\n",
    "mpl.rcParams['axes.edgecolor'] = 'black'\n",
    "mpl.rcParams['axes.titlesize'] = 25\n",
    "mpl.rcParams['legend.fontsize'] = 20\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "import torchvision.transforms.functional as TF\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be726116-2e2e-4c7e-9c25-f3f992329247",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707cbe3-a75c-4e43-8370-02787867ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-msssim\n",
    "#!pip install ptflops\n",
    "#!pip install fvcore torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4a471-f9a2-428d-9079-dba92018d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED);  np.random.seed(SEED)\n",
    "torch.manual_seed(SEED);  torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class CFG:\n",
    "    IMG_SZ      = 256          \n",
    "    BATCH       = 8\n",
    "    EPOCH_KD    = 8\n",
    "    EPOCH_BC    = 8\n",
    "    LAMBDA      = 0.5          \n",
    "    LR          = 0.00004\n",
    "    DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(DEVICE)\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66bdf7-43d1-4e8e-8efe-719480a2bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(frame_dir: str, hm_dir: str):\n",
    "#     frames, hmaps = [], []\n",
    "#     for fname in sorted(os.listdir(frame_dir)):\n",
    "#         f_path, h_path = os.path.join(frame_dir, fname), os.path.join(hm_dir, fname)\n",
    "\n",
    "#         frm  = cv2.imread(f_path)                      # BGR uint8\n",
    "#         hmap = cv2.imread(h_path, cv2.IMREAD_GRAYSCALE)   # (H,W) uint8\n",
    "#         if frm is None or hmap is None:\n",
    "#             print(f'[WARN] Could not load {fname}')\n",
    "#             continue\n",
    "\n",
    "#         frm  = cv2.resize(frm,  (cfg.IMG_SZ, cfg.IMG_SZ))\n",
    "#         hmap = cv2.resize(hmap,(cfg.IMG_SZ, cfg.IMG_SZ))\n",
    "\n",
    "#         frames.append(frm[..., ::-1])            # to RGB\n",
    "#         hmaps.append(hmap)\n",
    "\n",
    "#     return np.stack(frames,0), np.stack(hmaps,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0bd4c-c94e-4a67-94ca-33dacc372e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(frame_dir: str, hm_dir: str):\n",
    "    frames = []\n",
    "    hmaps = []\n",
    "\n",
    "    for fname in sorted(os.listdir(frame_dir)):\n",
    "        if fname.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        f_path = os.path.join(frame_dir, fname)\n",
    "        h_path = os.path.join(hm_dir, fname)\n",
    "\n",
    "        frm = cv2.imread(f_path)\n",
    "        hmap = cv2.imread(h_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if frm is None or hmap is None:\n",
    "            print(f'[WARN] Could not load {fname}')\n",
    "            continue\n",
    "\n",
    "        frm = cv2.resize(frm, (cfg.IMG_SZ, cfg.IMG_SZ))\n",
    "        hmap = cv2.resize(hmap, (cfg.IMG_SZ, cfg.IMG_SZ))\n",
    "\n",
    "        frames.append(frm[..., ::-1])  # BGR to RGB\n",
    "        hmaps.append(hmap)\n",
    "\n",
    "    return np.stack(frames), np.stack(hmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91ae85-556d-4b1b-8ccd-edbffde60c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_folder = 'Images/GT/extracted_frames'\n",
    "heatmap_folder = 'Images/Gaze/extracted_frames'\n",
    "\n",
    "\n",
    "frames, hmaps = load_data(frame_folder, heatmap_folder)\n",
    "print(frames.shape, hmaps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225fe79-b3ee-4109-909d-fa4055b5e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GazeDataset(Dataset):\n",
    "#     def __init__(self, frames, hmaps):\n",
    "#         self.f_t = transforms.Compose([\n",
    "#             transforms.ToPILImage(),\n",
    "#             transforms.ToTensor()\n",
    "#         ])\n",
    "#         self.h_t = transforms.Compose([\n",
    "#             transforms.ToPILImage(mode='L'),\n",
    "#             transforms.ToTensor()\n",
    "#         ])\n",
    "#         self.X, self.Y = frames, hmaps\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.f_t(self.X[idx]), self.h_t(self.Y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee2d9c-1b04-46d5-81b3-226c02195db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GazeDataset(Dataset):\n",
    "#     def __init__(self, frames, hmaps, augment=True):\n",
    "#         if augment:\n",
    "#             self.f_t = transforms.Compose([\n",
    "#                 transforms.ToPILImage(),\n",
    "#                 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])\n",
    "#             ])\n",
    "#         else:\n",
    "#             self.f_t = transforms.Compose([\n",
    "#                 transforms.ToPILImage(),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])\n",
    "#             ])\n",
    "\n",
    "#         self.h_t = transforms.Compose([\n",
    "#             transforms.ToPILImage(mode='L'),\n",
    "#             transforms.ToTensor()  # Keeps heatmap in (1, 256, 256)\n",
    "#         ])\n",
    "\n",
    "#         self.X, self.Y = frames, hmaps\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.f_t(self.X[idx]), self.h_t(self.Y[idx])\n",
    "\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, frames, hmaps, augment=True):\n",
    "        self.augment = augment\n",
    "        self.frames, self.hmaps = frames, hmaps\n",
    "\n",
    "     \n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2,\n",
    "            saturation=0.2, hue=0.05)\n",
    "\n",
    "        self.to_tensor_norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std =[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.frames[idx]         \n",
    "        hm  = self.hmaps [idx]          \n",
    "\n",
    "      \n",
    "        if self.augment:\n",
    "            if random.random() < 0.5: \n",
    "                img = np.fliplr(img).copy()\n",
    "                hm  = np.fliplr(hm ).copy()\n",
    "\n",
    "     \n",
    "        if self.augment:\n",
    "            img = self.color_jitter(TF.to_pil_image(img))\n",
    "\n",
    "        # to tensor\n",
    "        img = self.to_tensor_norm(img)\n",
    "        hm  = torch.from_numpy(hm).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        return img, hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12142427-7e42-4603-9e32-ccd8d3a32c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X, te_X, tr_Y, te_Y = train_test_split(\n",
    "    frames, hmaps, test_size=0.15, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(GazeDataset(tr_X, tr_Y), batch_size=cfg.BATCH, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(GazeDataset(te_X, te_Y), batch_size=cfg.BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3908d96-bd22-48bb-b3fd-3a3827020d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (frames, heatmaps) in enumerate(val_loader):\n",
    "    print(f\"Batch {idx + 1}\")\n",
    "    print(f\"Frames shape: {frames.shape}\")\n",
    "    print(f\"Heatmaps shape: {heatmaps.shape}\")\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635ebd6-010e-4419-80f8-c5beb4c71792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.channel_fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // 8, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // 8, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.spatial_conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ch_att = self.channel_fc(x)\n",
    "        x = x * ch_att\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        sp_att = self.spatial_conv(torch.cat([avg_out, max_out], dim=1))\n",
    "        return x * sp_att\n",
    "\n",
    "\n",
    "class TeacherNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=pretrained)\n",
    "        self.layer1 = nn.Sequential(*list(base.children())[:5])   # conv1 to layer1\n",
    "        self.layer2 = base.layer2\n",
    "        self.layer3 = base.layer3\n",
    "        self.layer4 = base.layer4\n",
    "\n",
    "        self.att4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SpatialChannelAttention(256)\n",
    "        )\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.att3 = SpatialChannelAttention(128 + 256)\n",
    "        self.up2 = nn.ConvTranspose2d(384, 64, 4, 2, 1)\n",
    "        self.att2 = SpatialChannelAttention(64 + 128)\n",
    "        self.up1 = nn.ConvTranspose2d(192, 32, 4, 2, 1)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.layer1(x)\n",
    "        f2 = self.layer2(f1)\n",
    "        f3 = self.layer3(f2)\n",
    "        f4 = self.layer4(f3)\n",
    "\n",
    "        x = self.att4(f4)\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, f3], dim=1)\n",
    "        x = self.att3(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, f2], dim=1)\n",
    "        att_map = self.att2(x)       \n",
    "        x = att_map\n",
    "        x = self.up1(x)\n",
    "\n",
    "        return self.final_up(x), att_map     \n",
    "\n",
    "\n",
    "# class StudentNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             SpatialChannelAttention(64)\n",
    "#         )\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 32, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             SpatialChannelAttention(32)\n",
    "#         )\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(32, 16, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             SpatialChannelAttention(16)\n",
    "#         )\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 8, 3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             SpatialChannelAttention(8)\n",
    "#         )\n",
    "#         self.heatmap = nn.Sequential(\n",
    "#             nn.Conv2d(8, 1, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         att_map = self.layer4(x)            \n",
    "#         heatmap = self.heatmap(att_map)\n",
    "\n",
    "#         return heatmap, att_map          \n",
    "\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "      \n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=2),  # 128x128\n",
    "            nn.ReLU(),\n",
    "            SpatialChannelAttention(64)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),  # 64x64\n",
    "            nn.ReLU(),\n",
    "            SpatialChannelAttention(128)\n",
    "        )\n",
    "\n",
    "   \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SpatialChannelAttention(256)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),  \n",
    "            nn.ReLU(),\n",
    "            SpatialChannelAttention(128)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),  \n",
    "            nn.ReLU(),\n",
    "            SpatialChannelAttention(64)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.heatmap = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(x1)\n",
    "        x_b = self.bottleneck(x2)\n",
    "        x_d1 = self.dec1(x_b)\n",
    "        x_d2 = self.dec2(x_d1)\n",
    "        heatmap = self.heatmap(x_d2)\n",
    "\n",
    "        att_map = x_b  \n",
    "\n",
    "        return heatmap, att_map\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "x = torch.randn(2, 3, 256, 256).to(device)\n",
    "\n",
    "teacher = TeacherNet(pretrained=False).to(device)\n",
    "student = StudentNet().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    t_map, t_att = teacher(x)\n",
    "    s_map, s_att = student(x)\n",
    "\n",
    "print(\"Teacher Heatmap:\", t_map.shape)\n",
    "print(\"Teacher Attention Map:\", t_att.shape)\n",
    "print(\"Student Heatmap:\", s_map.shape)\n",
    "print(\"Student Attention Map:\", s_att.shape)\n",
    "\n",
    "\n",
    "flops_teacher = FlopCountAnalysis(teacher, x).total()         \n",
    "flops_student = FlopCountAnalysis(student,  x).total()\n",
    "\n",
    "params_teacher = parameter_count(teacher)[\"\"]                \n",
    "params_student = parameter_count(student)[\"\"]\n",
    "\n",
    "\n",
    "flops_total   = flops_teacher  + flops_student\n",
    "params_total  = params_teacher + params_student\n",
    "\n",
    "\n",
    "def to_m(x):\n",
    "    return f\"{x/1e6:.2f}\"\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Model\"      : [\"TeacherNet\", \"StudentNet\", r\"\\textbf{MemGaze (Total)}\"],\n",
    "    \"FLOPs (M)\"  : [to_m(flops_teacher), to_m(flops_student),  to_m(flops_total)],\n",
    "    \"Params (M)\" : [to_m(params_teacher), to_m(params_student), to_m(params_total)]\n",
    "})\n",
    "\n",
    "print(df.to_markdown(index=False, tablefmt=\"github\"))\n",
    "\n",
    "latex = df.to_latex(index=False, escape=False, column_format=\"lcc\",\n",
    "                    caption=\"Computational cost of the proposed pipeline.\",\n",
    "                    label=\"tab:flops_params\")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a030e8-0560-4a8d-8670-4bc6109965fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_teacher(pred_map, gt_map, kind=\"smooth_l1\"):\n",
    " \n",
    "    if kind == \"l1\":\n",
    "        return F.l1_loss(pred_map, gt_map)\n",
    "    elif kind == \"smooth_l1\":\n",
    "        return F.smooth_l1_loss(pred_map, gt_map)\n",
    "    elif kind == \"mse\":\n",
    "        return F.mse_loss(pred_map, gt_map)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss type: choose from ['l1', 'smooth_l1', 'mse']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d0c0f-135c-4f86-bce5-f1325d1bbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_loss_ssim_kl(student_map, teacher_map, alpha=0.5, T=0.2):\n",
    "\n",
    "    B = student_map.size(0)\n",
    "    data_rng = max((teacher_map.max() - teacher_map.min()).item(), 1e-8)\n",
    "\n",
    "    ssim_loss = 1 - ssim(student_map, teacher_map, data_range=data_rng, size_average=True)\n",
    "\n",
    "    s_logp = F.log_softmax(student_map.view(B, -1) / T, dim=1)\n",
    "    t_prob = F.softmax(teacher_map.view(B, -1) / T, dim=1)\n",
    "    kl_loss = F.kl_div(s_logp, t_prob, reduction=\"batchmean\") * (T ** 2)\n",
    "\n",
    "    return (1 - alpha) * ssim_loss + alpha * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a4ac3-5c7c-407c-8d48-24f153557424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher_epoch(teacher, loader, optimizer, device):\n",
    "    teacher.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, heatmaps in tqdm(loader, desc=\"[Teacher Training]\"):\n",
    "        frames = frames.to(device).float()\n",
    "        heatmaps = heatmaps.to(device).float()\n",
    "        #print(frames.shape, heatmaps.shape)\n",
    "\n",
    "        pred_heatmap, _ = teacher(frames) \n",
    "\n",
    "        loss = F.mse_loss(pred_heatmap, heatmaps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_teacher(teacher, loader, device):\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, heatmaps in loader:\n",
    "        frames = frames.to(device).float()\n",
    "        heatmaps = heatmaps.to(device).float()\n",
    "\n",
    "        pred_heatmap, _ = teacher(frames)\n",
    "        loss = F.mse_loss(pred_heatmap, heatmaps)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69981e-480f-4394-80b1-1a780adb29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_epoch(student, teacher, loader, optimizer, device, alpha=0.5, T=0.75):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, _ in tqdm(loader, desc=\"[Student KD Training]\"):\n",
    "        frames = frames.to(device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_map, _ = teacher(frames)\n",
    "\n",
    "        s_map, _ = student(frames)\n",
    "\n",
    "        loss = kd_loss_ssim_kl(s_map, t_map, alpha=alpha, T=T)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_student(student, teacher, loader, device, alpha=0.4, T=1.5):\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, _ in loader:\n",
    "        frames = frames.to(device).float()\n",
    "\n",
    "        t_map, _ = teacher(frames)\n",
    "        s_map, _ = student(frames)\n",
    "\n",
    "        loss = F.mse_loss(s_map, t_map)#, alpha=alpha, T=T) kd_loss_ssim_kl\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2a004-abb8-47ba-b362-631ee41da422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "teacher = TeacherNet(pretrained=True).to(device)\n",
    "student = StudentNet().to(device)\n",
    "\n",
    "opt_t = torch.optim.Adam(teacher.parameters(), lr=1e-4)\n",
    "opt_s = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "alpha = 0.5\n",
    "T = 1.5\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "\n",
    "EPOCHS_T = 100\n",
    "for ep in range(1, EPOCHS_T + 1):\n",
    "    tr_loss = train_teacher_epoch(teacher, train_loader, opt_t, device)\n",
    "    val_loss = eval_teacher(teacher, val_loader, device)\n",
    "    print(f\"[Teacher {ep:02d}/{EPOCHS_T}] Train={tr_loss:.4f} | Val={val_loss:.4f}\")\n",
    "\n",
    "\n",
    "EPOCHS_S = 100\n",
    "for ep in range(1, EPOCHS_S + 1):\n",
    "    tr_loss_s = train_student_epoch(student, teacher, val_loader, opt_s, device)\n",
    "    val_loss_s = eval_student(student, teacher, train_loader, device)\n",
    "    train_losses.append(tr_loss_s)\n",
    "    val_losses.append(val_loss_s)\n",
    "    print(f\"[Student {ep:02d}/{EPOCHS_S}] KD-Train={tr_loss:.4f} | KD-Val={val_loss:.4f}\")\n",
    "    \n",
    "torch.save(student.state_dict(), \"student_kd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1785352-11f4-4e2b-90d7-63ca8e51f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(train_losses, label='Train Loss', linestyle='dashdot')#dotted')\n",
    "plt.plot(val_losses, label='Val Loss', linestyle='dotted')\n",
    "plt.xlabel('Epochs', weight=\"semibold\")\n",
    "plt.ylabel('Loss', weight=\"semibold\")\n",
    "plt.title(\"Student Knowledge Distillation Training Curve\", weight=\"semibold\")\n",
    "plt.legend(prop=dict(weight='semibold'))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543f9c7-5bb9-4e3e-9410-052f8386d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imitation_loss(student_map, gaze_map, kind=\"smooth_l1\"):\n",
    "    if kind == \"l1\":\n",
    "        return F.l1_loss(student_map, gaze_map)\n",
    "    elif kind == \"mse\":\n",
    "        return F.mse_loss(student_map, gaze_map)\n",
    "    elif kind == \"cosine\":\n",
    "\n",
    "        student_flat = student_map.view(student_map.size(0), -1)\n",
    "        gaze_flat = gaze_map.view(gaze_map.size(0), -1)\n",
    "        cos_sim = F.cosine_similarity(student_flat, gaze_flat, dim=1)\n",
    "        return torch.mean(1 - cos_sim)  # Loss = 1 - cosine similarity\n",
    "    else:\n",
    "        return F.smooth_l1_loss(student_map, gaze_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cc509-7cf1-4e64-ab95-afbb77d01a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_il_epoch(student, teacher, loader, optimizer, device, alpha_kd=0.7, beta_il=0.7, T=1.5):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, gaze_maps in tqdm(loader, desc=\"[Student KD+IL Training]\"):\n",
    "        frames = frames.to(device).float()\n",
    "        gaze_maps = gaze_maps.to(device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_map, _ = teacher(frames)\n",
    "\n",
    "        s_map, _ = student(frames)\n",
    "\n",
    "        loss_kd = kd_loss_ssim_kl(s_map, t_map, alpha=alpha_kd, T=T)\n",
    "        loss_il = imitation_loss(s_map, gaze_maps, kind=\"cosine\")\n",
    "\n",
    "        loss = loss_kd + beta_il * loss_il\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffcc17-a9e0-48d3-b29a-f1e75dc80a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "student.load_state_dict(torch.load(\"student_kd.pth\"))\n",
    "\n",
    "student_kd = copy.deepcopy(student)\n",
    "\n",
    "EPOCHS_IL = 100\n",
    "il_losses = []\n",
    "\n",
    "for ep in range(1, EPOCHS_IL + 1):\n",
    "    il_loss = train_student_il_epoch(student, teacher, train_loader, opt_s, device)\n",
    "    il_losses.append(il_loss)\n",
    "    print(f\"[Student (IL) {ep:02d}/{EPOCHS_IL}] Loss={il_loss:.4f}\")\n",
    "    \n",
    "torch.save(student.state_dict(), \"student_kd_il.pth\")\n",
    "student_kd_il = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255f5ce-c49b-4bcc-b09d-9d92d83888c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(train_losses, label='Student KD Train Loss', linestyle='solid')#dotted')\n",
    "#plt.plot(il_losses, label='Student KD + IL Train Loss', linestyle='solid')\n",
    "plt.xlabel('Epochs', weight=\"semibold\")\n",
    "#plt.ylabel('Loss', weight=\"semibold\")\n",
    "plt.ylabel(\"$T_{Loss}$\")\n",
    "#plt.title(\"Student Training Loss\", weight=\"semibold\")\n",
    "plt.legend(prop=dict(weight='semibold'))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd81d3-8323-4df7-9a14-296bba2e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_heatmap(hm):\n",
    "    hm = hm - hm.min()\n",
    "    return hm / (hm.max() + 1e-8)\n",
    "\n",
    "def denormalize_img(t):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    t = t.permute(1, 2, 0).cpu().numpy()  # (H,W,3)\n",
    "    img = (t * std) + mean\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "def plot_saliency_overlay_comparison(frames, gaze_maps, kd_maps, kd_il_maps, batch_idx=0, save_dir=\"compare_outputs\", prefix=\"multi\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    N = min(frames.size(0), 6)\n",
    "    fig, axs = plt.subplots(nrows=N, ncols=4, figsize=(20, 4.5 * N), dpi=300)\n",
    "\n",
    "    for i in range(N):\n",
    "        img = denormalize_img(frames[i])\n",
    "\n",
    "        gaze_map   = normalize_heatmap(gaze_maps[i, 0].cpu().numpy())\n",
    "        kd_map     = normalize_heatmap(kd_maps[i, 0].cpu().numpy())\n",
    "        kd_il_map  = normalize_heatmap(kd_il_maps[i, 0].cpu().numpy())\n",
    "\n",
    "        axs[i, 0].imshow(img)\n",
    "        axs[i, 0].set_title(\"Input\", fontsize=25, weight = 'semibold')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        axs[i, 1].imshow(img, alpha=1.0)\n",
    "        axs[i, 1].imshow(gaze_map, cmap='inferno', alpha=0.5)\n",
    "        axs[i, 1].set_title(\"GT Gaze\", fontsize=25, weight = 'semibold')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        axs[i, 2].imshow(img, alpha=1.0)\n",
    "        axs[i, 2].imshow(kd_map, cmap='jet', alpha=0.4)\n",
    "        axs[i, 2].set_title(\"Student (KD)\", fontsize=25, weight = 'semibold')\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "        axs[i, 3].imshow(img, alpha=1.0)\n",
    "        axs[i, 3].imshow(kd_il_map,  cmap='jet', alpha=0.4)\n",
    "        axs[i, 3].set_title(\"Student (KD+IL)\", fontsize=25, weight = 'semibold')\n",
    "        axs[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #out_path = os.path.join(save_dir, f\"{prefix}_batch{batch_idx}_overlay.png\")\n",
    "    #plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    #plt.close()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c314bf-f88f-4367-9770-ff5e12a2147b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "student_kd.eval()\n",
    "student_kd_il.eval()\n",
    "teacher.eval()\n",
    "\n",
    "MAX_BATCHES = 20  \n",
    "\n",
    "for b_idx, (frames, gaze_maps) in enumerate(train_loader):\n",
    "    if b_idx >= MAX_BATCHES:\n",
    "        break\n",
    "\n",
    "    frames = frames.to(device)\n",
    "    gaze_maps = gaze_maps.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, s_kd_maps    = student_kd(frames)\n",
    "        _, s_kd_il_maps = student_kd_il(frames)\n",
    "\n",
    "    plot_saliency_overlay_comparison(frames, gaze_maps, s_kd_maps, s_kd_il_maps,\n",
    "                                  batch_idx=b_idx,\n",
    "                                  save_dir=\"compare_outputs\",\n",
    "                                  prefix=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aea9d1-5f85-4aae-94a4-ab49168d7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_heatmap(hm):\n",
    "    hm = hm - hm.min()\n",
    "    hm = hm / (hm.max() + 1e-8)\n",
    "    return hm\n",
    "\n",
    "def plot_saliency_comparison(frames, gaze_maps, student_kd_maps, student_kd_il_maps, save_dir=\"compare_outputs\", prefix=\"comp\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(frames.size(0)):\n",
    "        img = frames[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "\n",
    "        gaze = normalize_heatmap(gaze_maps[i, 0].cpu().numpy())\n",
    "        kd_map = normalize_heatmap(student_kd_maps[i, 0].cpu().numpy())\n",
    "        kd_il_map = normalize_heatmap(student_kd_il_maps[i, 0].cpu().numpy())\n",
    "\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(14, 3))\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].set_title(\"Input\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(img, alpha=0.5)\n",
    "        axs[1].imshow(gaze)#, cmap='jet', alpha=0.5)\n",
    "        axs[1].set_title(\"Ground Truth (Gaze)\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        axs[2].imshow(img, alpha=0.)\n",
    "        axs[2].imshow(kd_map)#, cmap='jet', alpha=0.5)\n",
    "        axs[2].set_title(\"Student (KD)\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        axs[3].imshow(img, alpha=0.8)\n",
    "        axs[3].imshow(kd_il_map)#, cmap='jet', alpha=0.5)\n",
    "        axs[3].set_title(\"Student (KD+IL)\")\n",
    "        axs[3].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(os.path.join(save_dir, f\"{prefix}_{i}.png\"), dpi=150, bbox_inches='tight')\n",
    "        #plt.close()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d432800-110c-4d4d-8dd2-9fc953e9e413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "student_kd.eval()\n",
    "student_kd_il.eval()\n",
    "\n",
    "frames, gaze_maps = next(iter(val_loader))\n",
    "frames = frames.to(device)\n",
    "gaze_maps = gaze_maps.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, s_kd_maps = student_kd(frames)\n",
    "    _, s_kd_il_maps = student_kd_il(frames)\n",
    "\n",
    "plot_saliency_comparison(frames, gaze_maps, s_kd_maps, s_kd_il_maps, prefix=\"val_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fabf19-61d5-4950-a825-8d203a958702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_heatmap(hm):\n",
    "    hm = hm - hm.min()\n",
    "    return hm / (hm.max() + 1e-8)\n",
    "\n",
    "def plot_multiple_saliency_comparisons(frames, gaze_maps, kd_maps, kd_il_maps, N=6, save_dir=\"compare_outputs\", prefix=\"multi\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    N = min(N, frames.size(0))  \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=N, ncols=4, figsize=(14, 3 * N))\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        if axs.ndim == 1:\n",
    "            axs = axs.reshape((N, 4))\n",
    "    else:\n",
    "        axs = np.array([[axs]]) \n",
    "\n",
    "    for i in range(N):\n",
    "        img = frames[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img - img.min()) / (img.max() + 1e-8)\n",
    "\n",
    "        gaze     = normalize_heatmap(gaze_maps[i, 0].cpu().numpy())\n",
    "        kd_map   = normalize_heatmap(kd_maps[i, 0].cpu().numpy())\n",
    "        kd_il_map= normalize_heatmap(kd_il_maps[i, 0].cpu().numpy())\n",
    "\n",
    "        axs[i, 0].imshow(img)\n",
    "        axs[i, 0].set_title(\"Input\")\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        axs[i, 1].imshow(img, alpha=0.8)\n",
    "        axs[i, 1].imshow(gaze, alpha=0.5)#, cmap='jet', alpha=0.5)\n",
    "        axs[i, 1].set_title(\"GT Gaze\")\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        axs[i, 2].imshow(img, alpha=0.8)\n",
    "        axs[i, 2].imshow(kd_map, alpha=0.5)#, cmap='jet', alpha=0.5)\n",
    "        axs[i, 2].set_title(\"Student (KD)\")\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "        axs[i, 3].imshow(img, alpha=0.8)\n",
    "        axs[i, 3].imshow(kd_il_map, alpha=0.5)#, cmap='jet', alpha=0.5)\n",
    "        axs[i, 3].set_title(\"Student (KD+IL)\")\n",
    "        axs[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #out_path = os.path.join(save_dir, f\"{prefix}_samples.png\")\n",
    "    #plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d390a-ec45-4411-90ba-41f95a983ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_kd.eval()\n",
    "student_kd_il.eval()\n",
    "teacher.eval()\n",
    "\n",
    "frames, gaze_maps = next(iter(train_loader))\n",
    "#print(\"Batch size:\", frames.size(3))  # Add this\n",
    "\n",
    "frames = frames.to(device)\n",
    "gaze_maps = gaze_maps.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, s_kd_maps = student_kd(frames)\n",
    "    _, s_kd_il_maps = student_kd_il(frames)\n",
    "\n",
    "# Set N <= batch size\n",
    "plot_multiple_saliency_comparisons(frames, gaze_maps, s_kd_maps, s_kd_il_maps, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7c6e6-a0fd-4128-bf1e-20c6f3349ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "student_kd.eval()\n",
    "student_kd_il.eval()\n",
    "teacher.eval()\n",
    "\n",
    "MAX_BATCHES = 20  \n",
    "\n",
    "for b_idx, (frames, gaze_maps) in enumerate(val_loader):\n",
    "    if b_idx >= MAX_BATCHES:\n",
    "        break\n",
    "\n",
    "    frames = frames.to(device)\n",
    "    gaze_maps = gaze_maps.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, s_kd_maps    = student_kd(frames)\n",
    "        _, s_kd_il_maps = student_kd_il(frames)\n",
    "\n",
    "    plot_saliency_overlay_comparison(frames, gaze_maps, s_kd_maps, s_kd_il_maps,\n",
    "                                  batch_idx=b_idx,\n",
    "                                  save_dir=\"compare_outputs\",\n",
    "                                  prefix=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107523d1-66a2-4750-9f21-fe7dbfb87925",
   "metadata": {},
   "source": [
    "# Rough Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caed23c-dd4a-4d96-99a5-2a94e9de67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_teacher(pred_map, gt_map, kind=\"smooth_l1\"):\n",
    "    if kind == \"l1\":\n",
    "        return F.l1_loss(pred_map, gt_map)\n",
    "    elif kind == \"smooth_l1\":\n",
    "        return F.smooth_l1_loss(pred_map, gt_map)\n",
    "    else:\n",
    "        return F.mse_loss(pred_map, gt_map)\n",
    "\n",
    "\n",
    "def kd_loss_ssim_kl(student_map, teacher_map, alpha=0.4, T=1.5):\n",
    "    B = student_map.size(0)\n",
    "    data_rng = max((teacher_map.max() - teacher_map.min()).item(), 1e-8)  # Safe range\n",
    "    ssim_loss = 1 - ssim(student_map, teacher_map, data_range=data_rng, size_average=True)\n",
    "\n",
    "    s_logp = F.log_softmax(student_map.view(B, -1) / T, dim=1)\n",
    "    t_prob = F.softmax(teacher_map.view(B, -1) / T, dim=1)\n",
    "    kl_loss = F.kl_div(s_logp, t_prob, reduction=\"batchmean\") * (T ** 2)\n",
    "\n",
    "    return (1 - alpha) * ssim_loss + alpha * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a357cf-f60e-4292-ac5c-cc5c0ad50877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher_epoch(teacher, loader, opt, device):\n",
    "    teacher.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, heatmaps in tqdm(loader, desc=\"Teacher-Train\"):\n",
    "        frames = frames.to(device).float()\n",
    "        heatmaps = heatmaps.to(device).float()\n",
    "\n",
    "        pred_heatmap = teacher(frames)  # only heatmap\n",
    "\n",
    "        loss = F.smooth_l1_loss(pred_heatmap, heatmaps)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_teacher(teacher, loader, device):\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for frames, heatmaps in loader:\n",
    "        frames = frames.to(device).float()\n",
    "        heatmaps = heatmaps.to(device).float()\n",
    "\n",
    "        pred_heatmap = teacher(frames)\n",
    "        loss = F.mse_loss(pred_heatmap, heatmaps)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec81376-597b-4474-92bc-21c910f9b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "teacher = TeacherNet(pretrained=True).to(device)\n",
    "student = StudentNet().to(device)\n",
    "\n",
    "opt_t = torch.optim.Adam(teacher.parameters(), lr=1e-4)\n",
    "opt_s = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS_T = 50\n",
    "t_train, t_val = [], []\n",
    "\n",
    "for ep in range(1, EPOCHS_T + 1):\n",
    "    tr_loss = train_teacher_epoch(teacher, train_loader, opt_t, device)\n",
    "    val_loss = eval_teacher(teacher, val_loader, device)\n",
    "\n",
    "    t_train.append(tr_loss)\n",
    "    t_val.append(val_loss)\n",
    "\n",
    "    print(f\"[Teacher {ep:02d}/{EPOCHS_T}] TrainLoss={tr_loss:.4f} | ValLoss={val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c13e8-1809-41c3-ac2f-d3355b89a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3           \n",
    "log_every = 20        \n",
    "opt_teacher = torch.optim.Adam(teacher.parameters(), lr=4e-5)\n",
    "opt_student = torch.optim.Adam(student.parameters(), lr=4e-5)\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    teacher.train(); student.train()\n",
    "    tot_T, tot_S, tot_KD, tot_IM = 0, 0, 0, 0\n",
    "\n",
    "    for b_idx, (frames, gazes) in enumerate(train_loader, 1):\n",
    "        frames, gazes = frames.to(DEVICE), gazes.to(DEVICE)\n",
    "        B = frames.size(0)\n",
    "\n",
    "        pred_teacher   = teacher(frames)                  # (B,1,256,256)\n",
    "        t_loss         = loss_teacher(pred_teacher, gazes)\n",
    "\n",
    "        opt_teacher.zero_grad()\n",
    "        t_loss.backward()\n",
    "        opt_teacher.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_maps = pred_teacher.detach()         \n",
    "\n",
    "        pred_student   = student(frames)                \n",
    "        s_loss, kd_l, im_l = kd_loss_ssim_kl(\n",
    "                                pred_student, teacher_maps, gazes\n",
    "                             )\n",
    "\n",
    "        opt_student.zero_grad()\n",
    "        s_loss.backward()\n",
    "        opt_student.step()\n",
    "        tot_T  += t_loss.item()\n",
    "        tot_S  += s_loss.item()\n",
    "        tot_KD += kd_l\n",
    "        tot_IM += im_l\n",
    "\n",
    "        if b_idx % log_every == 0:\n",
    "            print(f\"  [ep{ep} | step {b_idx}] \"\n",
    "                  f\"T {t_loss.item():.4f} | S {s_loss.item():.4f} \"\n",
    "                  f\"(KD {kd_l:.4f}, IM {im_l:.4f})\")\n",
    "\n",
    "    N = len(train_loader)\n",
    "    print(f\"Epoch {ep}/{EPOCHS}  \"\n",
    "          f\"T {tot_T/N:.4f} | S {tot_S/N:.4f}  \"\n",
    "          f\"KD {tot_KD/N:.4f} | IM {tot_IM/N:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9f43b-43dd-43df-9abf-b125e5ad6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(X,T,S,G,count=2):\n",
    "    for i in range(min(count, X.size(0))):\n",
    "        fig,ax=plt.subplots(1,4,figsize=(14,3)); [a.axis('off') for a in ax]\n",
    "        ax[0].imshow(X[i].permute(1,2,0).cpu()); ax[0].set_title(\"Input\")\n",
    "        ax[1].imshow(T[i,0].cpu(),cmap='hot');   ax[1].set_title(\"Teacher\")\n",
    "        ax[2].imshow(S[i,0].cpu(),cmap='hot');   ax[2].set_title(\"Student\")\n",
    "        ax[3].imshow(G[i,0].cpu(),cmap='hot');   ax[3].set_title(\"GT Gaze\")\n",
    "        plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8ba45-c976-40bb-92e5-12bcb347fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, vis=True):\n",
    "    model.eval(); teacher.eval()\n",
    "    rhos, mses = [], []\n",
    "    for X,G in loader:\n",
    "        X,G = X.to(DEVICE), G.to(DEVICE)\n",
    "        S = model(X)\n",
    "        for i in range(X.size(0)):\n",
    "            s = S[i,0].cpu().numpy(); g = G[i,0].cpu().numpy()\n",
    "            rhos.append(spearmanr(s.flatten(), g.flatten()).correlation)\n",
    "            mses.append(np.mean((s-g)**2))\n",
    "        if vis:\n",
    "            show_heatmaps(X, teacher(X), S, G, count=2)\n",
    "            vis=False\n",
    "    print(f\"Spearman œÅ : {np.mean(rhos):.4f}\")\n",
    "    print(f\"MSE        : {np.mean(mses):.4f}\")\n",
    "\n",
    "evaluate(student, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c03810-b8ca-4a83-bfeb-a31daa68b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_sample(student, teacher, loader, device):\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    batch_iter = iter(loader)\n",
    "    frames, gazes = next(batch_iter)\n",
    "    frames = frames.to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        teacher_out = teacher(frames).squeeze(1).cpu().numpy()\n",
    "        student_out = student(frames).squeeze(1).cpu().numpy()\n",
    "\n",
    "    rgb = frames[0].permute(1, 2, 0).cpu().numpy()\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "\n",
    "    t_map = normalize_heatmap(teacher_out[0])\n",
    "    s_map = normalize_heatmap(student_out[0])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"Input Image\")\n",
    "\n",
    "    axs[1].imshow(rgb, alpha=0.5)\n",
    "    axs[1].imshow(t_map, cmap='jet', alpha=0.5)\n",
    "    axs[1].set_title(\"Teacher Heatmap\")\n",
    "\n",
    "    axs[2].imshow(rgb, alpha=0.5)\n",
    "    axs[2].imshow(s_map, cmap='jet', alpha=0.5)\n",
    "    axs[2].set_title(\"Student Heatmap\")\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(\"Saliency Transfer - Sanity Check\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
